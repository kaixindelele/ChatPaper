import numpy as np
import os
import re
import datetime
import arxiv
import openai, tenacity
import base64, requests
import argparse
import configparser
import fitz, io, os
from PIL import Image
import gradio
import markdown
import json
import tiktoken
import concurrent.futures
from optimizeOpenAI import chatPaper
import ipywidgets as widgets
from IPython.display import display

def parse_text(text):
    lines = text.split("\n")
    for i, line in enumerate(lines):
        if "```" in line:
            items = line.split('`')
            if items[-1]:
                lines[i] = f'<pre><code class="{items[-1]}">'
            else:
                lines[i] = f'</code></pre>'
        else:
            if i > 0:
                line = line.replace("<", "&lt;")
                line = line.replace(">", "&gt;")
                lines[i] = '<br/>' + line.replace(" ", "&nbsp;")
    return "".join(lines)


# def get_response(system, context, myKey, raw = False):
#     openai.api_key = myKey
#     response = openai.ChatCompletion.create(
#         model="gpt-3.5-turbo",
#         messages=[system, *context],
#     )
#     openai.api_key = ""
#     if raw:
#         return response
#     else:
#         message = response["choices"][0]["message"]["content"]
#         message_with_stats = f'{message}'
#         return message, parse_text(message_with_stats)

valid_api_keys = []


def api_key_check(api_key):
    try:
        chat = chatPaper([api_key])
        if chat.check_api_available():
            return api_key
        else:
            return None
    except:
        return None


def valid_apikey(api_keys):
    api_keys = api_keys.replace(' ', '')
    api_key_list = api_keys.split(',')
    print(api_key_list)
    global valid_api_keys
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_results = {
            executor.submit(api_key_check, api_key): api_key
            for api_key in api_key_list
        }
        for future in concurrent.futures.as_completed(future_results):
            result = future.result()
            if result:
                valid_api_keys.append(result)
    if len(valid_api_keys) > 0:
        return "æœ‰æ•ˆçš„api-keyä¸€å…±æœ‰{}ä¸ªï¼Œåˆ†åˆ«æ˜¯ï¼š{}, ç°åœ¨å¯ä»¥æäº¤ä½ çš„paper".format(
            len(valid_api_keys), valid_api_keys)
    return "æ— æ•ˆçš„api-key"


class Paper:

    def __init__(self, path, title='', url='', abs='', authers=[], sl=[]):
        # åˆå§‹åŒ–å‡½æ•°ï¼Œæ ¹æ®pdfè·¯å¾„åˆå§‹åŒ–Paperå¯¹è±¡
        self.url = url  # æ–‡ç« é“¾æ¥
        self.path = path  # pdfè·¯å¾„
        self.sl = sl
        self.section_names = []  # æ®µè½æ ‡é¢˜
        self.section_texts = {}  # æ®µè½å†…å®¹
        self.abs = abs
        self.title_page = 0
        if title == '':
            self.pdf = fitz.open(self.path)  # pdfæ–‡æ¡£
            self.title = self.get_title()
            self.parse_pdf()
        else:
            self.title = title
        self.authers = authers
        self.roman_num = [
            "I", "II", 'III', "IV", "V", "VI", "VII", "VIII", "IIX", "IX", "X"
        ]
        self.digit_num = [str(d + 1) for d in range(10)]
        self.first_image = ''

    def parse_pdf(self):
        self.pdf = fitz.open(self.path)  # pdfæ–‡æ¡£
        self.text_list = [page.get_text() for page in self.pdf]
        self.all_text = ' '.join(self.text_list)
        self.section_page_dict = self._get_all_page_index()  # æ®µè½ä¸é¡µç çš„å¯¹åº”å­—å…¸
        print("section_page_dict", self.section_page_dict)
        self.section_text_dict = self._get_all_page()  # æ®µè½ä¸å†…å®¹çš„å¯¹åº”å­—å…¸
        self.section_text_dict.update({"title": self.title})
        self.section_text_dict.update({"paper_info": self.get_paper_info()})
        self.pdf.close()

    def get_paper_info(self):
        first_page_text = self.pdf[self.title_page].get_text()
        if "Abstract" in self.section_text_dict.keys():
            abstract_text = self.section_text_dict['Abstract']
        else:
            abstract_text = self.abs
        introduction_text = self.section_text_dict['Introduction']
        first_page_text = first_page_text.replace(abstract_text, "").replace(
            introduction_text, "")
        return first_page_text

    def get_image_path(self, image_path=''):
        """
        å°†PDFä¸­çš„ç¬¬ä¸€å¼ å›¾ä¿å­˜åˆ°image.pngé‡Œé¢ï¼Œå­˜åˆ°æœ¬åœ°ç›®å½•ï¼Œè¿”å›æ–‡ä»¶åç§°ï¼Œä¾›giteeè¯»å–
        :param filename: å›¾ç‰‡æ‰€åœ¨è·¯å¾„ï¼Œ"C:\\Users\\Administrator\\Desktop\\nwd.pdf"
        :param image_path: å›¾ç‰‡æå–åçš„ä¿å­˜è·¯å¾„
        :return:
        """
        # open file
        max_size = 0
        image_list = []
        with fitz.Document(self.path) as my_pdf_file:
            # éå†æ‰€æœ‰é¡µé¢
            for page_number in range(1, len(my_pdf_file) + 1):
                # æŸ¥çœ‹ç‹¬ç«‹é¡µé¢
                page = my_pdf_file[page_number - 1]
                # æŸ¥çœ‹å½“å‰é¡µæ‰€æœ‰å›¾ç‰‡
                images = page.get_images()
                # éå†å½“å‰é¡µé¢æ‰€æœ‰å›¾ç‰‡
                for image_number, image in enumerate(page.get_images(),
                                                     start=1):
                    # è®¿é—®å›¾ç‰‡xref
                    xref_value = image[0]
                    # æå–å›¾ç‰‡ä¿¡æ¯
                    base_image = my_pdf_file.extract_image(xref_value)
                    # è®¿é—®å›¾ç‰‡
                    image_bytes = base_image["image"]
                    # è·å–å›¾ç‰‡æ‰©å±•å
                    ext = base_image["ext"]
                    # åŠ è½½å›¾ç‰‡
                    image = Image.open(io.BytesIO(image_bytes))
                    image_size = image.size[0] * image.size[1]
                    if image_size > max_size:
                        max_size = image_size
                    image_list.append(image)
        for image in image_list:
            image_size = image.size[0] * image.size[1]
            if image_size == max_size:
                image_name = f"image.{ext}"
                im_path = os.path.join(image_path, image_name)
                print("im_path:", im_path)

                max_pix = 480
                origin_min_pix = min(image.size[0], image.size[1])

                if image.size[0] > image.size[1]:
                    min_pix = int(image.size[1] * (max_pix / image.size[0]))
                    newsize = (max_pix, min_pix)
                else:
                    min_pix = int(image.size[0] * (max_pix / image.size[1]))
                    newsize = (min_pix, max_pix)
                image = image.resize(newsize)

                image.save(open(im_path, "wb"))
                return im_path, ext
        return None, None

    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®å­—ä½“çš„å¤§å°ï¼Œè¯†åˆ«æ¯ä¸ªç« èŠ‚åç§°ï¼Œå¹¶è¿”å›ä¸€ä¸ªåˆ—è¡¨
    def get_chapter_names(self, ):
        # # æ‰“å¼€ä¸€ä¸ªpdfæ–‡ä»¶
        doc = fitz.open(self.path)  # pdfæ–‡æ¡£
        text_list = [page.get_text() for page in doc]
        all_text = ''
        for text in text_list:
            all_text += text
        # # åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨ç« èŠ‚åç§°
        chapter_names = []
        for line in all_text.split('\n'):
            line_list = line.split(' ')
            if '.' in line:
                point_split_list = line.split('.')
                space_split_list = line.split(' ')
                if 1 < len(space_split_list) < 5:
                    if 1 < len(point_split_list) < 5 and (
                            point_split_list[0] in self.roman_num
                            or point_split_list[0] in self.digit_num):
                        print("line:", line)
                        chapter_names.append(line)

        return chapter_names

    def get_title(self):
        doc = self.pdf  # æ‰“å¼€pdfæ–‡ä»¶
        max_font_size = 0  # åˆå§‹åŒ–æœ€å¤§å­—ä½“å¤§å°ä¸º0
        max_string = ""  # åˆå§‹åŒ–æœ€å¤§å­—ä½“å¤§å°å¯¹åº”çš„å­—ç¬¦ä¸²ä¸ºç©º
        max_font_sizes = [0]
        for page_index, page in enumerate(doc):  # éå†æ¯ä¸€é¡µ
            text = page.get_text("dict")  # è·å–é¡µé¢ä¸Šçš„æ–‡æœ¬ä¿¡æ¯
            blocks = text["blocks"]  # è·å–æ–‡æœ¬å—åˆ—è¡¨
            for block in blocks:  # éå†æ¯ä¸ªæ–‡æœ¬å—
                if block["type"] == 0 and len(block['lines']):  # å¦‚æœæ˜¯æ–‡å­—ç±»å‹
                    if len(block["lines"][0]["spans"]):
                        font_size = block["lines"][0]["spans"][0][
                            "size"]  # è·å–ç¬¬ä¸€è¡Œç¬¬ä¸€æ®µæ–‡å­—çš„å­—ä½“å¤§å°
                        max_font_sizes.append(font_size)
                        if font_size > max_font_size:  # å¦‚æœå­—ä½“å¤§å°å¤§äºå½“å‰æœ€å¤§å€¼
                            max_font_size = font_size  # æ›´æ–°æœ€å¤§å€¼
                            max_string = block["lines"][0]["spans"][0][
                                "text"]  # æ›´æ–°æœ€å¤§å€¼å¯¹åº”çš„å­—ç¬¦ä¸²
        max_font_sizes.sort()
        print("max_font_sizes", max_font_sizes[-10:])
        cur_title = ''
        for page_index, page in enumerate(doc):  # éå†æ¯ä¸€é¡µ
            text = page.get_text("dict")  # è·å–é¡µé¢ä¸Šçš„æ–‡æœ¬ä¿¡æ¯
            blocks = text["blocks"]  # è·å–æ–‡æœ¬å—åˆ—è¡¨
            for block in blocks:  # éå†æ¯ä¸ªæ–‡æœ¬å—
                if block["type"] == 0 and len(block['lines']):  # å¦‚æœæ˜¯æ–‡å­—ç±»å‹
                    if len(block["lines"][0]["spans"]):
                        cur_string = block["lines"][0]["spans"][0][
                            "text"]  # æ›´æ–°æœ€å¤§å€¼å¯¹åº”çš„å­—ç¬¦ä¸²
                        font_flags = block["lines"][0]["spans"][0][
                            "flags"]  # è·å–ç¬¬ä¸€è¡Œç¬¬ä¸€æ®µæ–‡å­—çš„å­—ä½“ç‰¹å¾
                        font_size = block["lines"][0]["spans"][0][
                            "size"]  # è·å–ç¬¬ä¸€è¡Œç¬¬ä¸€æ®µæ–‡å­—çš„å­—ä½“å¤§å°
                        # print(font_size)
                        if abs(font_size - max_font_sizes[-1]) < 0.3 or abs(
                                font_size - max_font_sizes[-2]) < 0.3:
                            # print("The string is bold.", max_string, "font_size:", font_size, "font_flags:", font_flags)
                            if len(cur_string
                                   ) > 4 and "arXiv" not in cur_string:
                                # print("The string is bold.", max_string, "font_size:", font_size, "font_flags:", font_flags)
                                if cur_title == '':
                                    cur_title += cur_string
                                else:
                                    cur_title += ' ' + cur_string
                            self.title_page = page_index

        title = cur_title.replace('\n', ' ')
        return title

    def _get_all_page_index(self):
        # å®šä¹‰éœ€è¦å¯»æ‰¾çš„ç« èŠ‚åç§°åˆ—è¡¨
        section_list = self.sl
        # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ‰¾åˆ°çš„ç« èŠ‚å’Œå®ƒä»¬åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¡µç 
        section_page_dict = {}
        # éå†æ¯ä¸€é¡µæ–‡æ¡£
        for page_index, page in enumerate(self.pdf):
            # è·å–å½“å‰é¡µé¢çš„æ–‡æœ¬å†…å®¹
            cur_text = page.get_text()
            # éå†éœ€è¦å¯»æ‰¾çš„ç« èŠ‚åç§°åˆ—è¡¨
            for section_name in section_list:
                # å°†ç« èŠ‚åç§°è½¬æ¢æˆå¤§å†™å½¢å¼
                section_name_upper = section_name.upper()
                # å¦‚æœå½“å‰é¡µé¢åŒ…å«"Abstract"è¿™ä¸ªå…³é”®è¯
                if "Abstract" == section_name and section_name in cur_text:
                    # å°†"Abstract"å’Œå®ƒæ‰€åœ¨çš„é¡µç åŠ å…¥å­—å…¸ä¸­
                    section_page_dict[section_name] = page_index
                # å¦‚æœå½“å‰é¡µé¢åŒ…å«ç« èŠ‚åç§°ï¼Œåˆ™å°†ç« èŠ‚åç§°å’Œå®ƒæ‰€åœ¨çš„é¡µç åŠ å…¥å­—å…¸ä¸­
                else:
                    if section_name + '\n' in cur_text:
                        section_page_dict[section_name] = page_index
                    elif section_name_upper + '\n' in cur_text:
                        section_page_dict[section_name] = page_index
        # è¿”å›æ‰€æœ‰æ‰¾åˆ°çš„ç« èŠ‚åç§°åŠå®ƒä»¬åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¡µç 
        return section_page_dict

    def _get_all_page(self):
        """
        è·å–PDFæ–‡ä»¶ä¸­æ¯ä¸ªé¡µé¢çš„æ–‡æœ¬ä¿¡æ¯ï¼Œå¹¶å°†æ–‡æœ¬ä¿¡æ¯æŒ‰ç…§ç« èŠ‚ç»„ç»‡æˆå­—å…¸è¿”å›ã€‚
        Returns:
            section_dict (dict): æ¯ä¸ªç« èŠ‚çš„æ–‡æœ¬ä¿¡æ¯å­—å…¸ï¼Œkeyä¸ºç« èŠ‚åï¼Œvalueä¸ºç« èŠ‚æ–‡æœ¬ã€‚
        """
        text = ''
        text_list = []
        section_dict = {}

        # å†å¤„ç†å…¶ä»–ç« èŠ‚ï¼š
        text_list = [page.get_text() for page in self.pdf]
        for sec_index, sec_name in enumerate(self.section_page_dict):
            print(sec_index, sec_name, self.section_page_dict[sec_name])
            if sec_index <= 0 and self.abs:
                continue
            else:
                # ç›´æ¥è€ƒè™‘åé¢çš„å†…å®¹ï¼š
                start_page = self.section_page_dict[sec_name]
                if sec_index < len(list(self.section_page_dict.keys())) - 1:
                    end_page = self.section_page_dict[list(
                        self.section_page_dict.keys())[sec_index + 1]]
                else:
                    end_page = len(text_list)
                print("start_page, end_page:", start_page, end_page)
                cur_sec_text = ''
                if end_page - start_page == 0:
                    if sec_index < len(list(
                            self.section_page_dict.keys())) - 1:
                        next_sec = list(
                            self.section_page_dict.keys())[sec_index + 1]
                        if text_list[start_page].find(sec_name) == -1:
                            start_i = text_list[start_page].find(
                                sec_name.upper())
                        else:
                            start_i = text_list[start_page].find(sec_name)
                        if text_list[start_page].find(next_sec) == -1:
                            end_i = text_list[start_page].find(
                                next_sec.upper())
                        else:
                            end_i = text_list[start_page].find(next_sec)
                        cur_sec_text += text_list[start_page][start_i:end_i]
                else:
                    for page_i in range(start_page, end_page):
                        #                         print("page_i:", page_i)
                        if page_i == start_page:
                            if text_list[start_page].find(sec_name) == -1:
                                start_i = text_list[start_page].find(
                                    sec_name.upper())
                            else:
                                start_i = text_list[start_page].find(sec_name)
                            cur_sec_text += text_list[page_i][start_i:]
                        elif page_i < end_page:
                            cur_sec_text += text_list[page_i]
                        elif page_i == end_page:
                            if sec_index < len(
                                    list(self.section_page_dict.keys())) - 1:
                                next_sec = list(
                                    self.section_page_dict.keys())[sec_index +
                                                                   1]
                                if text_list[start_page].find(next_sec) == -1:
                                    end_i = text_list[start_page].find(
                                        next_sec.upper())
                                else:
                                    end_i = text_list[start_page].find(
                                        next_sec)
                                cur_sec_text += text_list[page_i][:end_i]
                section_dict[sec_name] = cur_sec_text.replace('-\n',
                                                              '').replace(
                                                                  '\n', ' ')
        return section_dict


# å®šä¹‰Readerç±»
class Reader:
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œè®¾ç½®å±æ€§
    def __init__(self,
                 key_word='',
                 query='',
                 filter_keys='',
                 root_path='./',
                 gitee_key='',
                 sort=arxiv.SortCriterion.SubmittedDate,
                 user_name='defualt',
                 language='cn',
                 api_keys: list = [],
                 model_name="gpt-3.5-turbo",
                 p=1.0,
                 temperature=1.0):
        self.api_keys = api_keys
        self.chatPaper = chatPaper(api_keys=self.api_keys,
                                   apiTimeInterval=10,
                                   temperature=temperature,
                                   top_p=p,
                                   model_name=model_name)  #openAI apiå°è£…
        self.user_name = user_name  # è¯»è€…å§“å
        self.key_word = key_word  # è¯»è€…æ„Ÿå…´è¶£çš„å…³é”®è¯
        self.query = query  # è¯»è€…è¾“å…¥çš„æœç´¢æŸ¥è¯¢
        self.sort = sort  # è¯»è€…é€‰æ‹©çš„æ’åºæ–¹å¼
        self.language = language  # è¯»è€…é€‰æ‹©çš„è¯­è¨€
        self.filter_keys = filter_keys  # ç”¨äºåœ¨æ‘˜è¦ä¸­ç­›é€‰çš„å…³é”®è¯
        self.root_path = root_path
        self.file_format = 'md'  # or 'txt'ï¼Œå¦‚æœä¸ºå›¾ç‰‡ï¼Œåˆ™å¿…é¡»ä¸º'md'
        self.save_image = False
        if self.save_image:
            self.gitee_key = self.config.get('Gitee', 'api')
        else:
            self.gitee_key = ''
        self.max_token_num = 4096
        self.encoding = tiktoken.get_encoding("gpt2")

    def get_arxiv(self, max_results=30):
        search = arxiv.Search(
            query=self.query,
            max_results=max_results,
            sort_by=self.sort,
            sort_order=arxiv.SortOrder.Descending,
        )
        return search

    def filter_arxiv(self, max_results=30):
        search = self.get_arxiv(max_results=max_results)
        print("all search:")
        for index, result in enumerate(search.results()):
            print(index, result.title, result.updated)

        filter_results = []
        filter_keys = self.filter_keys

        print("filter_keys:", self.filter_keys)
        # ç¡®ä¿æ¯ä¸ªå…³é”®è¯éƒ½èƒ½åœ¨æ‘˜è¦ä¸­æ‰¾åˆ°ï¼Œæ‰ç®—æ˜¯ç›®æ ‡è®ºæ–‡
        for index, result in enumerate(search.results()):
            abs_text = result.summary.replace('-\n', '-').replace('\n', ' ')
            meet_num = 0
            for f_key in filter_keys.split(" "):
                if f_key.lower() in abs_text.lower():
                    meet_num += 1
            if meet_num == len(filter_keys.split(" ")):
                filter_results.append(result)
                # break
        print("filter_results:", len(filter_results))
        print("filter_papers:")
        for index, result in enumerate(filter_results):
            print(index, result.title, result.updated)
        return filter_results

    def validateTitle(self, title):
        # å°†è®ºæ–‡çš„ä¹±ä¸ƒå…«ç³Ÿçš„è·¯å¾„æ ¼å¼ä¿®æ­£
        rstr = r"[\/\\\:\*\?\"\<\>\|]"  # '/ \ : * ? " < > |'
        new_title = re.sub(rstr, "_", title)  # æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        return new_title

    def download_pdf(self, filter_results):
        # å…ˆåˆ›å»ºæ–‡ä»¶å¤¹
        date_str = str(datetime.datetime.now())[:13].replace(' ', '-')
        key_word = str(self.key_word.replace(':', ' '))
        path = self.root_path + 'pdf_files/' + self.query.replace(
            'au: ', '').replace('title: ', '').replace('ti: ', '').replace(
                ':', ' ')[:25] + '-' + date_str
        try:
            os.makedirs(path)
        except:
            pass
        print("All_paper:", len(filter_results))
        # å¼€å§‹ä¸‹è½½ï¼š
        paper_list = []
        for r_index, result in enumerate(filter_results):
            try:
                title_str = self.validateTitle(result.title)
                pdf_name = title_str + '.pdf'
                # result.download_pdf(path, filename=pdf_name)
                self.try_download_pdf(result, path, pdf_name)
                paper_path = os.path.join(path, pdf_name)
                print("paper_path:", paper_path)
                paper = Paper(
                    path=paper_path,
                    url=result.entry_id,
                    title=result.title,
                    abs=result.summary.replace('-\n', '-').replace('\n', ' '),
                    authers=[str(aut) for aut in result.authors],
                )
                # ä¸‹è½½å®Œæ¯•ï¼Œå¼€å§‹è§£æï¼š
                paper.parse_pdf()
                paper_list.append(paper)
            except Exception as e:
                print("download_error:", e)
                pass
        return paper_list

    @tenacity.retry(wait=tenacity.wait_exponential(multiplier=1, min=4,
                                                   max=10),
                    stop=tenacity.stop_after_attempt(5),
                    reraise=True)
    def try_download_pdf(self, result, path, pdf_name):
        result.download_pdf(path, filename=pdf_name)

    @tenacity.retry(wait=tenacity.wait_exponential(multiplier=1, min=4,
                                                   max=10),
                    stop=tenacity.stop_after_attempt(5),
                    reraise=True)
    def upload_gitee(self, image_path, image_name='', ext='png'):
        """
        ä¸Šä¼ åˆ°ç äº‘
        :return:
        """
        with open(image_path, 'rb') as f:
            base64_data = base64.b64encode(f.read())
            base64_content = base64_data.decode()

        date_str = str(datetime.datetime.now())[:19].replace(':', '-').replace(
            ' ', '-') + '.' + ext
        path = image_name + '-' + date_str

        payload = {
            "access_token": self.gitee_key,
            "owner": self.config.get('Gitee', 'owner'),
            "repo": self.config.get('Gitee', 'repo'),
            "path": self.config.get('Gitee', 'path'),
            "content": base64_content,
            "message": "upload image"
        }
        # è¿™é‡Œéœ€è¦ä¿®æ”¹æˆä½ çš„giteeçš„è´¦æˆ·å’Œä»“åº“åï¼Œä»¥åŠæ–‡ä»¶å¤¹çš„åå­—ï¼š
        url = f'https://gitee.com/api/v5/repos/' + self.config.get(
            'Gitee', 'owner') + '/' + self.config.get(
                'Gitee', 'repo') + '/contents/' + self.config.get(
                    'Gitee', 'path') + '/' + path
        rep = requests.post(url, json=payload).json()
        print("rep:", rep)
        if 'content' in rep.keys():
            image_url = rep['content']['download_url']
        else:
            image_url = r"https://gitee.com/api/v5/repos/" + self.config.get(
                'Gitee', 'owner') + '/' + self.config.get(
                    'Gitee', 'repo') + '/contents/' + self.config.get(
                        'Gitee', 'path') + '/' + path

        return image_url


    def summary_with_chat(self, paper_list):
        htmls = []
        utoken = 0
        ctoken = 0
        ttoken = 0
        for paper_index, paper in enumerate(paper_list):
            # ç¬¬ä¸€æ­¥å…ˆç”¨titleï¼Œabsï¼Œå’Œintroductionè¿›è¡Œæ€»ç»“ã€‚
            text = ''
            text += 'Title:' + paper.title
            text += 'Url:' + paper.url
            text += 'Abstrat:' + paper.abs
            text += 'Paper_info:' + paper.section_text_dict['paper_info']
            # intro
            text += list(paper.section_text_dict.values())[0]
            #max_token = 2500 * 4
            #text = text[:max_token]
            chat_summary_text, utoken1, ctoken1, ttoken1 = self.chat_summary(
                text=text)
            htmls.append(chat_summary_text)

            # TODO å¾€mdæ–‡æ¡£ä¸­æ’å…¥è®ºæ–‡é‡Œçš„åƒç´ æœ€å¤§çš„ä¸€å¼ å›¾ç‰‡ï¼Œè¿™ä¸ªæ–¹æ¡ˆå¯ä»¥å¼„çš„æ›´åŠ æ™ºèƒ½ä¸€äº›ï¼š
            method_key = ''
            for parse_key in paper.section_text_dict.keys():
                if 'method' in parse_key.lower(
                ) or 'approach' in parse_key.lower():
                    method_key = parse_key
                    break

            if method_key != '':
                text = ''
                method_text = ''
                summary_text = ''
                summary_text += "<summary>" + chat_summary_text
                # methods
                method_text += paper.section_text_dict[method_key]
                text = summary_text + "\n<Methods>:\n" + method_text
                chat_method_text, utoken2, ctoken2, ttoken2 = self.chat_method(
                    text=text)
            else:
                chat_method_text = ''
            htmls.append(chat_method_text)
            htmls.append("\n")

            # ç¬¬ä¸‰æ­¥æ€»ç»“å…¨æ–‡ï¼Œå¹¶æ‰“åˆ†ï¼š
            conclusion_key = ''
            for parse_key in paper.section_text_dict.keys():
                if 'conclu' in parse_key.lower():
                    conclusion_key = parse_key
                    break

            text = ''
            conclusion_text = ''
            summary_text = ''
            summary_text += "<summary>" + chat_summary_text + "\n <Method summary>:\n" + chat_method_text
            if conclusion_key != '':
                # conclusion
                conclusion_text += paper.section_text_dict[conclusion_key]
                text = summary_text + "\n <Conclusion>:\n" + conclusion_text
            else:
                text = summary_text
            chat_conclusion_text, utoken3, ctoken3, ttoken3 = self.chat_conclusion(
                text=text)
            htmls.append(chat_conclusion_text)
            htmls.append("\n")
            # tokenç»Ÿè®¡
            utoken = utoken + utoken1 + utoken2 + utoken3
            ctoken = ctoken + ctoken1 + ctoken2 + ctoken3
            ttoken = ttoken + ttoken1 + ttoken2 + ttoken3
            cost = (ttoken / 1000) * 0.002
            pos_count = {
                "usage_token_used": str(utoken),
                "completion_token_used": str(ctoken),
                "total_token_used": str(ttoken),
                "cost": str(cost),
            }
            md_text = "\n".join(htmls)
            return markdown.markdown(md_text), pos_count

    @tenacity.retry(wait=tenacity.wait_exponential(multiplier=1, min=4,
                                                   max=10),
                    stop=tenacity.stop_after_attempt(5),
                    reraise=True)
    def chat_conclusion(self, text):
        conclusion_prompt_token = 650
        text_token = len(self.encoding.encode(text))
        clip_text_index = int(
            len(text) * (self.max_token_num - conclusion_prompt_token) /
            text_token)
        clip_text = text[:clip_text_index]
        self.chatPaper.reset(
            convo_id="chatConclusion",
            system_prompt="You are a reviewer in the field of [" +
            self.key_word + "] and you need to critically review this article")
        self.chatPaper.add_to_conversation(
            convo_id="chatConclusion",
            role="assistant",
            message=
            "This is the <summary> and <conclusion> part of an English literature, where <summary> you have already summarized, but <conclusion> part, I need your help to summarize the following questions:"
            + clip_text)  # èƒŒæ™¯çŸ¥è¯†ï¼Œå¯ä»¥å‚è€ƒOpenReviewçš„å®¡ç¨¿æµç¨‹
        content = """                 
                 8. Make the following summary.Be sure to use Chinese answers (proper nouns need to be marked in English).
                    - (1):What is the significance of this piece of work?
                    - (2):Summarize the strengths and weaknesses of this article in three dimensions: innovation point, performance, and workload.                   
                    .......
                 Follow the format of the output later: 
                 8. Conclusion: \n\n
                    - (1):xxx;\n                     
                    - (2):Innovation point: xxx; Performance: xxx; Workload: xxx;\n                      
                 
                 Be sure to use Chinese answers (proper nouns need to be marked in English), statements as concise and academic as possible, do not repeat the content of the previous <summary>, the value of the use of the original numbers, be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \n line feed, ....... means fill in according to the actual requirements, if not, you can not write.                 
                 """
        result = self.chatPaper.ask(
            prompt=content,
            role="user",
            convo_id="chatConclusion",
        )
        print(result)
        return result[0], result[1], result[2], result[3]

    @tenacity.retry(wait=tenacity.wait_exponential(multiplier=1, min=4,
                                                   max=10),
                    stop=tenacity.stop_after_attempt(5),
                    reraise=True)
    def chat_method(self, text):
        method_prompt_token = 650
        text_token = len(self.encoding.encode(text))
        clip_text_index = int(
            len(text) * (self.max_token_num - method_prompt_token) /
            text_token)
        clip_text = text[:clip_text_index]
        self.chatPaper.reset(
            convo_id="chatMethod",
            system_prompt="You are a researcher in the field of [" +
            self.key_word +
            "] who is good at summarizing papers using concise statements"
        )  # chatgpt è§’è‰²
        self.chatPaper.add_to_conversation(
            convo_id="chatMethod",
            role="assistant",
            message=str(
                "This is the <summary> and <Method> part of an English document, where <summary> you have summarized, but the <Methods> part, I need your help to read and summarize the following questions."
                + clip_text))
        content = """                 
                 7. Describe in detail the methodological idea of this article. Be sure to use Chinese answers (proper nouns need to be marked in English). For example, its steps are.
                    - (1):...
                    - (2):...
                    - (3):...
                    - .......
                 Follow the format of the output that follows: 
                 7. Methods: \n\n
                    - (1):xxx;\n 
                    - (2):xxx;\n 
                    - (3):xxx;\n  
                    ....... \n\n     
                 
                 Be sure to use Chinese answers (proper nouns need to be marked in English), statements as concise and academic as possible, do not repeat the content of the previous <summary>, the value of the use of the original numbers, be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \n line feed, ....... means fill in according to the actual requirements, if not, you can not write.                 
                 """
        result = self.chatPaper.ask(
            prompt=content,
            role="user",
            convo_id="chatMethod",
        )
        print(result)
        return result[0], result[1], result[2], result[3]

    @tenacity.retry(wait=tenacity.wait_exponential(multiplier=1, min=4,
                                                   max=10),
                    stop=tenacity.stop_after_attempt(5),
                    reraise=True)
    def chat_summary(self, text):
        summary_prompt_token = 1000
        text_token = len(self.encoding.encode(text))
        clip_text_index = int(
            len(text) * (self.max_token_num - summary_prompt_token) /
            text_token)
        clip_text = text[:clip_text_index]
        self.chatPaper.reset(
            convo_id="chatSummary",
            system_prompt="You are a researcher in the field of [" +
            self.key_word +
            "] who is good at summarizing papers using concise statements")
        self.chatPaper.add_to_conversation(
            convo_id="chatSummary",
            role="assistant",
            message=str(
                "This is the title, author, link, abstract and introduction of an English document. I need your help to read and summarize the following questions: "
                + clip_text))
        content = """                 
                 1. Mark the title of the paper (with Chinese translation)
                 2. list all the authors' names (use English)
                 3. mark the first author's affiliation (output Chinese translation only)                 
                 4. mark the keywords of this article (use English)
                 5. link to the paper, Github code link (if available, fill in Github:None if not)
                 6. summarize according to the following four points.Be sure to use Chinese answers (proper nouns need to be marked in English)
                    - (1):What is the research background of this article?
                    - (2):What are the past methods? What are the problems with them? Is the approach well motivated?
                    - (3):What is the research methodology proposed in this paper?
                    - (4):On what task and what performance is achieved by the methods in this paper? Can the performance support their goals?
                 Follow the format of the output that follows:                  
                 1. Title: xxx\n\n
                 2. Authors: xxx\n\n
                 3. Affiliation: xxx\n\n                 
                 4. Keywords: xxx\n\n   
                 5. Urls: xxx or xxx , xxx \n\n      
                 6. Summary: \n\n
                    - (1):xxx;\n 
                    - (2):xxx;\n 
                    - (3):xxx;\n  
                    - (4):xxx.\n\n     
                 
                 Be sure to use Chinese answers (proper nouns need to be marked in English), statements as concise and academic as possible, do not have too much repetitive information, numerical values using the original numbers, be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \n line feed.                 
                 """
        result = self.chatPaper.ask(
            prompt=content,
            role="user",
            convo_id="chatSummary",
        )
        print(result)
        return result[0], result[1], result[2], result[3]

    def export_to_markdown(self, text, file_name, mode='w'):
        # ä½¿ç”¨markdownæ¨¡å—çš„convertæ–¹æ³•ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºhtmlæ ¼å¼
        # html = markdown.markdown(text)
        # æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶ï¼Œä»¥å†™å…¥æ¨¡å¼
        with open(file_name, mode, encoding="utf-8") as f:
            # å°†htmlæ ¼å¼çš„å†…å®¹å†™å…¥æ–‡ä»¶
            f.write(text)

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œæ‰“å°å‡ºè¯»è€…ä¿¡æ¯
    def show_info(self):
        print(f"Key word: {self.key_word}")
        print(f"Query: {self.query}")
        print(f"Sort: {self.sort}")


def upload_pdf(api_keys, text, model_name, p, temperature, file):
    # æ£€æŸ¥ä¸¤ä¸ªè¾“å…¥éƒ½ä¸ä¸ºç©º
    api_key_list = None
    if api_keys:
        api_key_list = api_keys.split(',')
    elif not api_keys and valid_api_keys != []:
        api_key_list = valid_api_keys
    if not text or not file or not api_key_list:
        return "ä¸¤ä¸ªè¾“å…¥éƒ½ä¸èƒ½ä¸ºç©ºï¼Œè¯·è¾“å…¥å­—ç¬¦å¹¶ä¸Šä¼  PDF æ–‡ä»¶ï¼"

    # åˆ¤æ–­PDFæ–‡ä»¶
    #if file and file.name.split(".")[-1].lower() != "pdf":
    #    return 'è¯·å‹¿ä¸Šä¼ é PDF æ–‡ä»¶ï¼'
    else:
        section_list = text.split(',')
        paper_list = [Paper(path=file, sl=section_list)]
        # åˆ›å»ºä¸€ä¸ªReaderå¯¹è±¡
        print(api_key_list)
        reader = Reader(api_keys=api_key_list,
                        model_name=model_name,
                        p=p,
                        temperature=temperature)
        sum_info, cost = reader.summary_with_chat(
            paper_list=paper_list)  # type: ignore
        return cost, sum_info


api_title = "api-keyå¯ç”¨éªŒè¯"
api_description = '''<div align='left'>
<img src='https://visitor-badge.laobi.icu/badge?page_id=https://huggingface.co/spaces/wangrongsheng/ChatPaper'>
<img align='right' src='https://i.328888.xyz/2023/03/12/vH9dU.png' width="150">
Use ChatGPT to summary the papers.Star our Github [ğŸŒŸChatPaper](https://github.com/kaixindelele/ChatPaper) .
ğŸ’—å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼ğŸ’—
ğŸ”´è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
</div>
'''

api_input = [
    gradio.inputs.Textbox(label="è¯·è¾“å…¥ä½ çš„API-key(å¿…å¡«, å¤šä¸ªAPI-keyè¯·ç”¨è‹±æ–‡é€—å·éš”å¼€)",
                          default="",
                          type='password')
]
api_gui = gradio.Interface(fn=valid_apikey,
                           inputs=api_input,
                           outputs="text",
                           title=api_title,
                           description=api_description)

# æ ‡é¢˜
title = "ChatPaper"
# æè¿°
description = '''<div align='left'>
<img src='https://visitor-badge.laobi.icu/badge?page_id=https://huggingface.co/spaces/wangrongsheng/ChatPaper'>
<img align='right' src='https://i.328888.xyz/2023/03/12/vH9dU.png' width="150">
Use ChatGPT to summary the papers.Star our Github [ğŸŒŸChatPaper](https://github.com/kaixindelele/ChatPaper) .
ğŸ’—å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼ğŸ’—
ğŸ”´è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼
</div>
'''
# åˆ›å»ºGradioç•Œé¢
ip = [
    gradio.inputs.Textbox(label="è¯·è¾“å…¥ä½ çš„API-key(å¿…å¡«, å¤šä¸ªAPI-keyè¯·ç”¨è‹±æ–‡é€—å·éš”å¼€),ä¸éœ€è¦ç©ºæ ¼",
                          default="",
                          type='password'),
    gradio.inputs.Textbox(
        label="è¯·è¾“å…¥è®ºæ–‡å¤§æ ‡é¢˜ç´¢å¼•(ç”¨è‹±æ–‡é€—å·éš”å¼€,å¿…å¡«)",
        default=
        "'Abstract,Introduction,Related Work,Background,Preliminary,Problem Formulation,Methods,Methodology,Method,Approach,Approaches,Materials and Methods,Experiment Settings,Experiment,Experimental Results,Evaluation,Experiments,Results,Findings,Data Analysis,Discussion,Results and Discussion,Conclusion,References'"
    ),
    gradio.inputs.Radio(choices=["gpt-3.5-turbo", "gpt-3.5-turbo-0301"],
                        default="gpt-3.5-turbo",
                        label="Select model"),
    gradio.inputs.Slider(minimum=-0,
                         maximum=1.0,
                         default=1.0,
                         step=0.05,
                         label="Top-p (nucleus sampling)"),
    gradio.inputs.Slider(minimum=-0,
                         maximum=5.0,
                         default=0.5,
                         step=0.5,
                         label="Temperature"),
    gradio.inputs.File(label="è¯·ä¸Šä¼ è®ºæ–‡PDF(å¿…å¡«)")
]


chatpaper_gui = gradio.Interface(fn=upload_pdf,
                                 inputs=ip,
                                 outputs=["json", "html"],
                                 title=title,
                                 description=description)

# Start server
gui = gradio.TabbedInterface(interface_list=[api_gui, chatpaper_gui],
                             tab_names=["API-key", "ChatPaper"])
gui.launch(quiet=True, show_api=False)
